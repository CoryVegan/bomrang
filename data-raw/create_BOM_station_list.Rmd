---
title: "Create Databases of BoM Station Locations and JSON URLs"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document provides details on methods used to create the database of BoM
JSON files for stations and corresponding metadata, _e.g._, latitude, longitude
(which are more detailed than what is in the JSON file), start, end, elevation,
etc.

Refer to these BoM pages for more reference:

- http://www.bom.gov.au/inside/itb/dm/idcodes/struc.shtml

- http://reg.bom.gov.au/catalogue/data-feeds.shtml

- http://reg.bom.gov.au/catalogue/anon-ftp.shtml

- http://www.bom.gov.au/climate/cdo/about/site-num.shtml

## Product code definitions

### States

- IDD - NT

- IDN - NSW/ACT

- IDQ - Qld

- IDS - SA

- IDT - Tas/Antarctica (distinguished by the product number)

- IDV - Vic

- IDW - WA

### Product code numbers

- 60701 - coastal observations (duplicated in 60801)

- 60801 - all weather observations (we will use this)

- 60803 - Antarctica weather observations (and use this, this distinguishes
          Tas from Antarctica)

- 60901 - capital city weather observations (duplicated in 60801)

- 60903 - Canberra area weather observations (duplicated in 60801)

## Get station metadata

The station metadata are downloaded from a zip file linked from the 
"[Bureau of Meteorology Site Numbers](http://www.bom.gov.au/climate/cdo/about/site-num.shtml)"
website. The zip file may be directly downloaded, 
[file of site details](ftp://ftp.bom.gov.au/anon2/home/ncc/metadata/sitelists/stations.zip).

```{r get_bom_station_data, eval=TRUE}

library(magrittr)

# This file is a pseudo-fixed width file. Line five contains the headers at
# fixed widths which are coded in the read_table() call.
# The last six lines contain other information that we don't want.
# For some reason, reading it directly from the BoM website does not work, so
# we use download.file to fetch it first and then import it from the R
# tempdir()

curl::curl_download(
  url = "ftp://ftp.bom.gov.au/anon2/home/ncc/metadata/sitelists/stations.zip",
  destfile = file.path(tempdir(), "stations.zip"))

bom_stations_raw <-
  readr::read_table(
    file.path(tempdir(), "stations.zip"),
    skip = 4,
    na = c("..", ".....", " "),
    col_names = c(
      "site",
      "dist",
      "name",
      "start",
      "end",
      "lat",
      "lon",
      "NULL1",
      "NULL2",
      "state",
      "elev",
      "bar_ht",
      "wmo"
    )
  )

# remove extra columns for source of location
bom_stations_raw <- bom_stations_raw[, -c(8:9)]

# trim the end of the rows off that have extra info that's not in columns
nrows <- nrow(bom_stations_raw) - 6
bom_stations_raw <- bom_stations_raw[1:nrows, ]

# return only current stations listing
bom_stations_raw <-
  bom_stations_raw[is.na(bom_stations_raw$end), ]
bom_stations_raw$end <- format(Sys.Date(), "%Y")

bom_stations_raw
```

## Check that station locations

Occasionally the stations are listed in the wrong location, e.g. Alice Springs
Airport in SA. Perform quality check to ensure that the station locations are
accurate based on the lat/lon values provided.

```{r check-locations}
library(sf)
library(raster)

points <- st_as_sf(x = bom_stations_raw, 
                   coords = c("lon", "lat"),
                   crs = "+proj=longlat +datum=WGS84") %>%
  st_transform(., 3576)

Oz <- st_as_sf(raster::getData(name = "GADM",
                               country = "AUS",
                               level = 1)) %>% 
  st_transform(., 3576)

# check which state points fall in
bom_locations <- 
  st_join(points, Oz)

# join the new data from checking points with the BOM data
bom_locations <- dplyr::full_join(bom_stations_raw, bom_locations)
```

## Create state codes

Using the state values extracted from GADM to set state codes from BOM rather
than the sometimes incorrect `state` column from BOM.

BOM state codes are as follows:

- IDD - NT,

- IDN - NSW/ACT,

- IDQ - Qld,

- IDS - SA,

- IDT - Tas/Antarctica,

- IDV - Vic, and

- IDW - WA

```{r state-codes, message=FALSE}

# rename original state column from BOM and keep for reference
bom_locations$org_state <- bom_locations$state

# create new list of corrected state abbreviations based on spatial check
bom_locations$state <- substr(bom_locations$HASC_1, 4, 5)

# recode states from two to three letters where needed
bom_locations$state[bom_locations$state == "QL"] <- "QLD"
bom_locations$state[bom_locations$state == "VI"] <- "VIC"
bom_locations$state[bom_locations$state == "TS"] <- "TAS"
bom_locations$state[bom_locations$state == "NS"] <- "NSW"
bom_locations$state[bom_locations$state == "CT"] <- "ACT"

# fill any states not present in corrected set
bom_locations$state[is.na(bom_locations$state)] <- 
  bom_locations$state[is.na(bom_locations$state)]

# replace state values with state code to generate URLs
bom_locations$state_code <- NA
bom_locations$state_code[bom_locations$state == "WA"] <- "W"
bom_locations$state_code[bom_locations$state == "QL" |
                           bom_locations$state == "QLD"] <- "Q"
bom_locations$state_code[bom_locations$state == "VI" |
                           bom_locations$state == "VIC"] <- "V"
bom_locations$state_code[bom_locations$state == "NT"] <- "D"
bom_locations$state_code[bom_locations$state == "TS" |
                           bom_locations$state == "TAS"] <- "T"
bom_locations$state_code[bom_locations$state == "NS" |
                           bom_locations$state == "NSW"] <- "N"
bom_locations$state_code[bom_locations$state == "SA"] <- "S"
bom_locations$state_code[bom_locations$state == "ANT"] <- "T"

# generate URLs
stations_site_list <-
  bom_locations %>%
  dplyr::select(site:wmo, org_state, state, state_code) %>%
  dplyr::mutate(
    url = dplyr::case_when(
      .$state != "ANT" & !is.na(.$wmo) ~
        paste0(
          "http://www.bom.gov.au/fwo/ID",
          .$state_code,
          "60801",
          "/",
          "ID",
          .$state_code,
          "60801",
          ".",
          .$wmo,
          ".json"
        ),
      .$state == "ANT" & !is.na(.$wmo) ~
        paste0(
          "http://www.bom.gov.au/fwo/ID",
          .$state_code,
          "60803",
          "/",
          "ID",
          .$state_code,
          "60803",
          ".",
          .$wmo,
          ".json"
        )
    )
  )

```

## Save data

Now that we have the data frame of stations and have generated the URLs for the
JSON files for stations providing weather data feeds, save the data as a
database for _bomrang_ to use.

There are weather stations that do have a WMO but don't report online, e.g.,
KIRIBATI NTC AWS or MARSHALL ISLANDS NTC AWS, in this section remove these from
the list and then create a database for use with the current weather information
from BoM.

### Save JSON URL database for `get_current_weather()`

```{r save_url_data, eval=TRUE, message=FALSE}
JSONurl_site_list <-
  stations_site_list[!is.na(stations_site_list$url), ]

JSONurl_site_list <-
  JSONurl_site_list %>%
  dplyr::rowwise() %>%
  dplyr::mutate(url = dplyr::if_else(httr::http_error(url), NA_character_, url))
  
# Remove new NA values from invalid URLs and convert to data.table
JSONurl_site_list <-
  data.table::data.table(stations_site_list[!is.na(stations_site_list$url), ])

 if (!dir.exists("../inst/extdata")) {
      dir.create("../inst/extdata", recursive = TRUE)
    }

# Save database
  save(JSONurl_site_list,
       file = "../inst/extdata/JSONurl_site_list.rda",
     compress = "bzip2")
```

### Save station location data for `get_ag_bulletin()`

First, rename columns and drop a few that aren't necessary for the ag bulletin
information. Then pad the `site` field with 0 to match the data in the XML file
that holds the bulletin information. Lastly, create the database for use in the
package.

```{r save_location_data, eval=TRUE, message=FALSE}
stations_site_list <-
  stations_site_list %>%
  dplyr::select(-state_code, -url) %>% 
  as.data.frame()

stations_site_list$site <-
  gsub("^0{1,2}", "", stations_site_list$site)

  save(stations_site_list, file = "../inst/extdata/stations_site_list.rda",
     compress = "bzip2")
```

# Cleanup

# Cleanup the GADM file

```{r cleanup}
unlink("GADM_2.8_AUS_adm1.rds")
```

## Session Info
```{r session_info, echo=FALSE}
sessioninfo::session_info()
```
